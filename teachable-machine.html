<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sound Classifier Project</title>
    <link rel="stylesheet" href="stylepage.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.6.0/p5.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.6.0/addons/p5.sound.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/ml5@0.10.2/dist/ml5.min.js"></script>
    <script src="script.js" defer></script>
</head>
<body>
    <header>
        <h1>Sound Classifier: Detecting Clapping Gestures</h1><br>
        <p>A project inspired by 'Unmasking AI' by Joy Buolamwini</p>
    </header>

    <main>
        <!-- Interactive Model Section -->
        <section id="interactive-model">
            <h2>Try the Sound Classifier</h2>
            <button id="start-button">Start Classifying</button>
            <div id="label-container"></div>
<!-- Add this block below your existing Start Classifying button -->
<div style="text-align: center; margin-top: 15px;">
  <p style="font-size: 16px; color: #333; line-height: 1.8; max-width: 600px; margin: 0 auto;">
    <strong style="color: #007BFF;">1. Single Clap:</strong> Recognizes a single, isolated clap.
    <br><strong style="color: #28A745;">2. Double Fast Clap:</strong> Identifies two quick, consecutive claps.
    <br><strong style="color: #FFC107;">3. Continuous Clapping:</strong> Detects an ongoing series of claps.
    <br><strong style="color: #DC3545;">4. Triple Clap:</strong> Recognizes three distinct claps in sequence.
  </p>
  <p style="margin-top: 15px; font-size: 16px; color: #333;">
    This feature demonstrates how sound classification can be used to enhance interactivity and accessibility.
  </p>
</div>

        </section>

        <!-- Project Statement Section -->
<section id="project-statement">
    <h2 style="text-align: center; color: #0056b3; margin-bottom: 20px;">
        Project Statement
    </h2>
    <div style="display: grid; grid-template-columns: 1fr; gap: 20px; max-width: 1200px; margin: 0 auto; padding: 20px;">

<!-- Add your image here -->
<img src="Screenshot 2024-12-02 at 1.32.13‚ÄØPM.png" alt="Clapping Gesture Example" 
     style="display: block; margin: 20px auto; border: 1px solid #ddd; border-radius: 8px; max-width: 100%; height: auto;" />

<div class="section">

</div>

        <!-- Purpose Section -->
        <div style="background-color: #f0f8ff; border: 1px solid #ddd; border-radius: 8px; padding: 20px;">
            <h3 style="color: #0056b3; text-align: center; margin-bottom: 10px;">Purpose</h3>
            <p>

<div class="chat-container">
  <!-- Wei's Chat Bubble 1 -->
  <div class="chat-message left">
    <img src="wei_zhang.jpg" alt="Wei Zhang" class="avatar">
    <div class="chat-bubble">
      <p>The üîäSound Classifier: Detecting Clapping Gesturesüîä project is designed to explore machine learning in an interactive, user-friendly, and socially impactful way. It focuses on identifying four distinct clapping gestures: single clap, continuous clapping, double fast clap, and triple clap. While these gestures may seem simple and playful, they hold potential for practical applications, particularly in the realm of accessibility.</p>
    </div>
  </div>

  <!-- Wei's Chat Bubble 2 -->
  <div class="chat-message left">
    <img src="wei_zhang.jpg" alt="Wei Zhang" class="avatar">
    <div class="chat-bubble">
      <p>It offers an intuitive way for individuals with disabilities, such as the visually impaired, to interact with devices through clapping patterns, like turning on lights or signaling for help. It also enhances smart home systems by enabling hands-free, gesture-based control. Additionally, it serves as an educational tool for demonstrating machine learning concepts and has potential in interactive entertainment and rehabilitation exercises, showcasing the versatility and inclusivity of AI. These considerations are directly inspired by Joy Buolamwini‚Äôs Unmasking AI.</p>
    </div>
  </div>
</div>


<!-- Add your image here -->
<img src="Screenshot 2024-12-02 at 1.45.21‚ÄØPM.png" alt="Clapping Gesture Example" 
     style="display: block; margin: 20px auto; border: 1px solid #ddd; border-radius: 8px; max-width: 70%; height: auto;" />

        <!-- Challenges Section -->
        <div style="background-color: #e6f7ff; border: 1px solid #ddd; border-radius: 8px; padding: 20px;">
            <h3 style="color: #0056b3; text-align: center; margin-bottom: 10px;">Challenges</h3>
            <ul style="margin-left: 20px;">

                <div class="chat-container">
  <!-- Wei's Chat Bubble 1 -->
  <div class="chat-message right">
    <img src="wei_zhang.jpg" alt="Wei Zhang" class="avatar">
    <div class="chat-bubble">
      <p>üìä<strong>Data Collection:</strong> Clapping sounds vary significantly depending on individual style, environment, and even cultural norms. A single clap from one person may be sharp and loud, while another person‚Äôs might be softer and slower. To address this, we aimed to collect data from multiple sources and conditions, but ensuring diversity while maintaining consistency in labeling was time-intensive and challenging.</p>
    </div>
  </div>

  <!-- Wei's Chat Bubble 2 -->
  <div class="chat-message right">
    <img src="wei_zhang.jpg" alt="Wei Zhang" class="avatar">
    <div class="chat-bubble">
      <p>üîä<strong>Noise Interference:</strong> Background noise was another major issue during both training and testing. Sounds like typing, footsteps, or conversations often interfered with the model‚Äôs ability to correctly classify clapping gestures. For example, a rapid sequence of background noise could occasionally be misclassified as continuous clapping. This limitation underscored the importance of incorporating noise data into the training process to enhance the model‚Äôs robustness.</p>
    </div>
  </div>

  <!-- Wei's Chat Bubble 3 -->
  <div class="chat-message right">
    <img src="wei_zhang.jpg" alt="Wei Zhang" class="avatar">
    <div class="chat-bubble">
      <p>üîß<strong>Model Accuracy:</strong> The model's accuracy heavily relied on the quality of the training data, as Teachable Machine does not allow for advanced customization of model architecture. During testing, we observed that the classifier occasionally struggled to differentiate between similar gestures, such as a fast double clap and the start of continuous clapping. This was particularly apparent when users varied their speed or rhythm inconsistently. While this limitation highlighted the simplicity of Teachable Machine‚Äôs approach, it also served as a reminder that more advanced platforms could offer additional tools for fine-tuning and improving classification performance.</p>
    </div>
  </div>

  <!-- Ziwei's Chat Bubble 1 -->
  <div class="chat-message left">
    <img src="ziwei.jpg" alt="Ziwei Yu" class="avatar">
    <div class="chat-bubble">
      <p>ü§î<strong>Ethical Considerations:</strong> Inspired by Joy Buolamwini‚Äôs <em>Unmasking AI</em>, we were conscious of ensuring that the model worked effectively for a wide range of users. For instance, physical differences in clapping styles due to age, ability, or cultural background could unintentionally bias the model if these variations were not adequately represented in the dataset. This realization reinforced the importance of designing AI systems that prioritize fairness and inclusivity, even in seemingly simple applications like sound classification.</p>
    </div>
  </div>

  <!-- Ziwei's Chat Bubble 2 -->
  <div class="chat-message left">
    <img src="ziwei.jpg" alt="Ziwei Yu" class="avatar">
    <div class="chat-bubble">
      <p>‚úã<strong>Deployment Challenges:</strong> Deploying the model in a web-based environment presented additional challenges. Ensuring compatibility across devices and browsers required iterative testing and debugging. For instance, certain browsers handled microphone permissions differently, causing inconsistent behavior during testing. Addressing these issues required both technical troubleshooting and user-focused design adjustments to streamline the user experience.</p>
    </div>
  </div>
</div>


<!-- Insert the image -->
<div style="text-align: center; margin: 20px 0;">
    <img src="./12345.png" alt="Description of the image" style="max-width: 70%; height: auto; border-radius: 8px;">
  </div>  
  

        <!-- Lessons from Unmasking AI Section -->
        <div style="background-color: #f8f9fa; border: 1px solid #ddd; border-radius: 8px; padding: 20px;">
            <h3 style="color: #0056b3; text-align: center; margin-bottom: 10px;">Lessons from <em>Unmasking AI</em></h3>
<div class="chat-container">
  <!-- Wei's Chat Bubble 1 -->
  <div class="chat-message right">
    <img src="wei_zhang.jpg" alt="Wei Zhang" class="avatar">
    <div class="chat-bubble">
      <p>‚ùï<strong>The Importance of Inclusive Datasets:</strong> One of the central themes in <em>Unmasking AI</em> is the role of inclusive datasets in ensuring fairness in machine learning systems. Buolamwini highlights how biased datasets can reinforce systemic inequalities, often excluding or misrepresenting certain groups. For this project, this lesson was especially significant when creating the training dataset for clapping gestures. To address this, we prioritized collecting diverse examples and ensured the classifier could perform reasonably well across different user scenarios.</p>
    </div>
  </div>

  <!-- Ziwei's Chat Bubble 1 -->
  <div class="chat-message left">
    <img src="ziwei.jpg" alt="Ziwei Yu" class="avatar">
    <div class="chat-bubble">
      <p>‚ùì<strong>Bias in Training and Testing:</strong> Another critical insight from the book is the subtle yet pervasive ways bias can infiltrate AI systems, even when they appear neutral. This was evident during the testing phase of our classifier. For instance, training the model exclusively in quiet environments inadvertently led to reduced performance in noisier settings, a clear example of environmental bias. This limitation reinforced the need for diverse training conditions that reflect the complexities of real-world use cases.</p>
    </div>
  </div>

  <!-- Wei's Chat Bubble 2 -->
  <div class="chat-message right">
    <img src="wei_zhang.jpg" alt="Wei Zhang" class="avatar">
    <div class="chat-bubble">
      <p>‚ôøÔ∏è<strong>Accessibility and User Experience:</strong> Buolamwini discusses how AI tools often fail to consider the needs of marginalized communities, leaving many users excluded from the benefits of these technologies. Inspired by this critique, we designed the Sound Classifier to be accessible and user-friendly. The interface is straightforward, requiring minimal technical expertise, and uses clear visual indicators (e.g., emojis) to display results. By focusing on sound as an input medium, the project also reduces reliance on visual interfaces, making it potentially more inclusive for users with visual impairments.</p>
    </div>
  </div>

  <!-- Ziwei's Chat Bubble 2 -->
  <div class="chat-message left">
    <img src="ziwei.jpg" alt="Ziwei Yu" class="avatar">
    <div class="chat-bubble">
      <p>ü™õ<strong>Ethical Responsibility of Developers:</strong> A recurring theme in <em>Unmasking AI</em> is the ethical responsibility of developers to consider the societal impact of their work. Even seemingly small design decisions‚Äîsuch as how data is labeled or how thresholds are set‚Äîcan have far-reaching consequences for fairness and accessibility. In this project, decisions about defining gesture categories or setting classification thresholds were guided by a commitment to inclusivity and fairness. For instance, we avoided overly narrow definitions of clapping gestures to ensure the classifier could accommodate variations in user input.</p>
    </div>
  </div>

  <!-- Wei's Chat Bubble 3 -->
  <div class="chat-message right">
    <img src="wei_zhang.jpg" alt="Wei Zhang" class="avatar">
    <div class="chat-bubble">
      <p>‚öôÔ∏è<strong>Broader Implications of AI:</strong> Buolamwini‚Äôs book also challenged us to think critically about the broader societal implications of AI. The Sound Classifier, while a relatively simple application, served as a microcosm of larger debates around AI ethics. It reminded us that even small-scale projects carry ethical weight, particularly when they involve user interaction and data collection.</p>
    </div>
  </div>
</div>


</div>

<!-- Insert the image -->
<div style="text-align: center; margin: 20px 0;">
    <img src="./Screenshot 2024-12-02 at 2.04.00‚ÄØPM.png" alt="Description of the image" style="max-width: 70%; height: auto; border-radius: 8px;">
  </div>
  

        <!-- Future Improvements Section -->
        <div style="background-color: #fff9e6; border: 1px solid #ddd; border-radius: 8px; padding: 20px;">
            <h3 style="color: #0056b3; text-align: center; margin-bottom: 10px;">Future Improvements</h3>
            <ul style="margin-left: 20px;">
<div class="chat-container">
  <!-- Wei's Chat Bubble 1 -->
  <div class="chat-message right">
    <img src="wei_zhang.jpg" alt="Wei Zhang" class="avatar">
    <div class="chat-bubble">
      <p>üìä<strong>Expanding the Dataset:</strong> One of the most impactful ways to improve the classifier is by expanding its dataset. While the current dataset includes diverse examples of clapping gestures, there is room to further enhance inclusivity. For instance, collecting clapping sounds from users of different age groups, cultural backgrounds, and physical abilities could help the model better generalize to real-world scenarios. Additionally, incorporating more subtle variations in clapping styles‚Äîsuch as softer claps, rhythmic claps, or claps with overlapping background noise‚Äîwould make the model more robust and adaptable.</p>
    </div>
  </div>

  <!-- Wei's Chat Bubble 2 -->
  <div class="chat-message right">
    <img src="wei_zhang.jpg" alt="Wei Zhang" class="avatar">
    <div class="chat-bubble">
      <p>üîä<strong>Noise Robustness:</strong> Background noise remains a significant challenge for the classifier. Future iterations could include deliberate noise augmentation during the training process. For example, adding synthetic or recorded ambient noise to the dataset would help the model distinguish between clapping gestures and environmental sounds. Additionally, testing the classifier in noisy settings, such as public spaces, classrooms, or outdoor environments, would further refine its robustness. Leveraging advanced noise-canceling algorithms or pre-processing techniques could enhance the classifier‚Äôs ability to isolate clapping sounds from competing noises.</p>
    </div>
  </div>

  <!-- Ziwei's Chat Bubble -->
  <div class="chat-message left">
    <img src="ziwei.jpg" alt="Ziwei Yu" class="avatar">
    <div class="chat-bubble">
      <p>ü§î<strong>Customization Beyond Teachable Machine:</strong> While Teachable Machine offers a simplified interface for building machine learning models, it has limitations in terms of customization. Moving the project to more advanced platforms, such as TensorFlow or PyTorch, could enable greater control over model architecture, hyperparameters, and feature engineering. This would allow for a deeper exploration of sound features and more precise classification, ultimately increasing the model's versatility and effectiveness.</p>
    </div>
  </div>
</div>


<!-- Insert the image -->
<div style="text-align: center; margin: 20px 0;">
    <img src="./Screenshot 2024-12-02 at 2.03.53‚ÄØPM.png" alt="Description of the image" style="max-width: 0%; height: auto; border-radius: 8px;">
  </div>
  
        <!-- Conclusion Section -->
        <div style="background-color: #f9f9f9; border: 1px solid #ddd; border-radius: 8px; padding: 20px;">
            <h3 style="color: #0056b3; text-align: center; margin-bottom: 10px;">Conclusion</h3>
    </p>
    <p>
        One of the project‚Äôs most significant takeaways was the importance of designing AI systems that are fair, inclusive, and accountable. Drawing on insights from Joy Buolamwini‚Äôs <em>Unmasking AI</em>, the project highlighted the ethical responsibilities of developers to prioritize equity and transparency. From the inclusivity of the dataset to the accessibility of the user interface, each design choice was a reflection of these values. Although challenges such as noise interference and dataset diversity presented limitations, they also served as opportunities for growth and critical reflection.
    </p>
    <p>
        The potential applications of the Sound Classifier extend far beyond its current scope. As highlighted in the <strong>Future Improvements</strong> section, integrating the classifier into assistive technologies or expanding its capabilities to recognize other sound-based gestures could have a profound impact on users with disabilities. For example, visually impaired individuals could benefit from gesture-based control systems that provide greater independence and convenience. These possibilities underscore the transformative power of machine learning when it is designed thoughtfully and inclusively.
    </p>
    <p>
</div>
</section>


        <!-- Video Embedding Section -->
        <div style="background-color: #fff; border: 1px solid #ddd; border-radius: 8px; padding: 20px;">
            <h3 style="color: #0056b3; text-align: center; margin-bottom: 10px;">Video Demonstration</h3>
            <p style="text-align: center;">üëÄWatch the algorithm in action below‚¨áÔ∏è:</p>
            <div style="text-align: center;">
                <iframe width="560" height="315" src="https://www.youtube.com/embed/TOrVsLklltM" title="Teachable Machine Sound Classifier" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </div>
        </div>
    </div>
</section>

        <!-- Attachments Section -->
        <section id="attachments">

<br>
<br>
<h2>Influences and References</h2>
<div style="line-height: 1.5;">
    <p>This project drew inspiration from The Coding Train's tutorial on sound classification.</p>
    <p style="text-align: center;">You can view their tutorial ‚û°Ô∏è <a href="https://thecodingtrain.com/tracks/teachable-machine/teachable-machine/3-sound-classification" target="_blank">here</a>.</p>
  </div> 

  <!-- Add GitHub Repository Links -->
<div style="text-align: center; margin-top: 40px;">
  <h3 style="color: navy;">GitHub Repositories</h3>
  <p style="font-size: 16px;">
    Current Project Repository: 
    <a href="https://github.com/wzhang729/lis500.git" target="_blank" style="color: blue; text-decoration: underline;">https://github.com/wzhang729/lis500.git</a>
  </p>
  <p style="font-size: 16px;">
    Previous Version Repository: 
    <a href="https://github.com/sjcoombsnovii/sjcoombsnovii.github.io.git" target="_blank" style="color: blue; text-decoration: underline;">https://github.com/sjcoombsnovii/sjcoombsnovii.github.io.git</a>
  </p>
</div>

</p>

        </section>
        
    </main>

    <footer>
        <p>Created by Wei Zhang as part of LIS500</p>
        <a href="index.html">Back to Home</a>
    </footer>
</body>
</html>
