<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sound Classifier Project</title>
    <link rel="stylesheet" href="stylepage.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.6.0/p5.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.6.0/addons/p5.sound.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/ml5@0.10.2/dist/ml5.min.js"></script>
    <script src="script.js" defer></script>
</head>
<body>
 
  <div id="top"></div>

    <header>
        <h1>Sound Classifier: Detecting Clapping Gestures</h1><br>
        <p>A project inspired by 'Unmasking AI' by Joy Buolamwini</p>
    </header>

    <main>
        <!-- Interactive Model Section -->
        <section id="interactive-model">
            <h2>Try the Sound Classifier</h2>
            <button id="start-button">Start Classifying</button>
            <div id="label-container"></div>

<!-- Add this block below your existing Start Classifying button -->
<div style="text-align: center; margin-top: 15px;">
  <p style="font-size: 16px; color: #333; line-height: 1.8; max-width: 600px; margin: 0 auto;">
    <strong style="color: #007BFF;">1. Single Clap:</strong> Recognizes a single, isolated clap.
    <br><strong style="color: #28A745;">2. Double Fast Clap:</strong> Identifies two quick, consecutive claps.
    <br><strong style="color: #FFC107;">3. Continuous Clapping:</strong> Detects an ongoing series of claps.
    <br><strong style="color: #DC3545;">4. Triple Clap:</strong> Recognizes three distinct claps in sequence.
  </p>
  <p style="margin-top: 15px; font-size: 16px; color: #333;">
In this test, ‚Äúclapping‚Äù is defined as a set of distinct hand-produced sound patterns. However, clapping is not universally expressed or understood in the same way across different cultures. Recognizing these variations ensures our model remains flexible, respectful, and inclusive of diverse human expressions.
  </p>
</div>

        </section>

        <!-- Project Statement Section -->
<section id="project-statement">
    <h2 style="text-align: center; color: #0056b3; margin-bottom: 20px;">
        Project Statement
    </h2>
    <div style="display: grid; grid-template-columns: 1fr; gap: 20px; max-width: 1200px; margin: 0 auto; padding: 20px;">

<!-- Add your image here -->
<img src="Screenshot 2024-12-09 at 4.45.25‚ÄØPM.png" alt="Clapping Gesture Example" 
     style="display: block; margin: 20px auto; border: 1px solid #ddd; border-radius: 8px; max-width: 100%; height: auto;" />

<div class="section">

</div>

        <!-- Purpose Section -->
        <div style="background-color: #f0f8ff; border: 1px solid #ddd; border-radius: 8px; padding: 20px;">
            <h3 style="color: #0056b3; text-align: center; margin-bottom: 10px;">Purpose</h3>
            <p>

<div class="chat-container">
  <!-- Wei's Chat Bubble 1 -->
  <div class="chat-message left">
    <img src="wei_zhang.jpg" alt="Wei Zhang" class="avatar">
    <div class="chat-bubble">
      <p>The Sound Classifier: Detecting Clapping Gestures project aims to demonstrate how machine learning can recognize and differentiate between various clapping patterns‚Äîsingle clap, continuous clapping, fast double clap, and triple clap‚Äîand to reflect on the social, cultural, and ethical dimensions of AI development. While these gestures may appear simple and playful, their potential applications span from accessibility enhancements for individuals with visual impairments to hands-free device control in smart homes, as well as interactive educational tools. By focusing on sound-based input, we can broaden AI‚Äôs inclusivity, particularly for users who may find traditional visual interfaces challenging. These goals resonate with themes discussed in Joy Buolamwini‚Äôs Unmasking AI, which encourages critical thought about fairness, representation, and cultural sensitivity in algorithmic design.</p>
    </div>
  </div>

<!-- Add your image here -->
<img src="Screenshot 2024-12-09 at 5.27.04‚ÄØPM.png" alt="Clapping Gesture Example" 
     style="display: block; margin: 20px auto; border: 1px solid #ddd; border-radius: 8px; max-width: 70%; height: auto;" />

        <!-- Challenges Section -->
        <div style="background-color: #e6f7ff; border: 1px solid #ddd; border-radius: 8px; padding: 20px;">
            <h3 style="color: #0056b3; text-align: center; margin-bottom: 10px;">Challenges</h3>
            <ul style="margin-left: 20px;">

                <div class="chat-container">
  <!-- Wei's Chat Bubble 1 -->
  <div class="chat-message right">
    <img src="wei_zhang.jpg" alt="Wei Zhang" class="avatar">
    <div class="chat-bubble">
      <p>üìä<strong>Data Collection:</strong> Clapping sounds vary across individuals, environments, and cultural backgrounds. Initially, our dataset was limited in its diversity. For example, most samples came from a single environment and a limited set of participants. As we refined the project based on feedback, we expanded our dataset to include clapping from individuals of different ages, genders, and cultural backgrounds, recorded in varied sound environments. This was time-intensive but essential for ensuring that the model does not inadvertently favor one particular style or context, thereby reducing hidden biases.</p>
    </div>
  </div>

  <!-- Wei's Chat Bubble 2 -->
  <div class="chat-message right">
    <img src="wei_zhang.jpg" alt="Wei Zhang" class="avatar">
    <div class="chat-bubble">
      <p>üîä<strong>Noise Interference:</strong> Background sounds such as typing, footsteps, and conversations sometimes interfered with classification. Initially, training data was recorded mostly in quiet environments, leading to poorer performance in noisier settings. To address this, we introduced ambient noise samples and recordings from busy spaces, ensuring that the classifier could better distinguish between clapping gestures and environmental sounds. While not perfect, these efforts aligned with principles from Unmasking AI that caution against overly narrow training conditions that fail to represent real-world complexity.</p>
    </div>
  </div>

  <!-- Wei's Chat Bubble 3 -->
  <div class="chat-message right">
    <img src="wei_zhang.jpg" alt="Wei Zhang" class="avatar">
    <div class="chat-bubble">
      <p>üîß<strong>Model Accuracy:</strong> Teachable Machine offers a simplified approach to model training. Early iterations struggled to differentiate between fast double claps and the onset of continuous clapping, particularly when user rhythm varied. By broadening the dataset and incorporating more nuanced, culturally and physically varied samples, we improved the model‚Äôs ability to generalize. The simplicity of Teachable Machine also underscored the need for potentially more advanced platforms in the future, where we can fine-tune architectures and hyperparameters for better feature extraction.</p>
    </div>
  </div>

  <!-- Ziwei's Chat Bubble 1 -->
  <div class="chat-message left">
    <img src="ziwei.jpg" alt="Ziwei Yu" class="avatar">
    <div class="chat-bubble">
      <p>ü§î<strong>Ethical Considerations:</strong> Unmasking AI highlights how biases in data can reinforce systemic inequalities. In this project, if we did not carefully consider cultural, age-related, or ability-based variations in clapping styles, our model risked excluding certain users. The feedback we received encouraged us to more explicitly integrate cultural relevance. We expanded our data collection efforts, inviting individuals from different communities, and reflected on how subtle differences in clapping could represent distinct cultural practices. By doing so, we aimed to create a classifier that not only recognized claps but did so in a way that did not center one normative pattern at the expense of others.</p>
    </div>
  </div>

  <!-- Ziwei's Chat Bubble 2 -->
  <div class="chat-message left">
    <img src="ziwei.jpg" alt="Ziwei Yu" class="avatar">
    <div class="chat-bubble">
      <p>‚úã<strong>Deployment Challenges:</strong> Ensuring browser and device compatibility required iterative testing and adjustments. Different browsers handle microphone permissions and input inconsistently, creating unpredictable user experiences. We integrated user-centered design principles, simplifying the output indicators and clarifying instructions. This not only streamlined interaction but also reinforced the importance of making tools accessible to as many users as possible, another central theme in Buolamwini‚Äôs work.</p>
    </div>
  </div>
</div>


<!-- Insert the image -->
<div style="text-align: center; margin: 20px 0;">
    <img src="Screenshot 2024-12-09 at 5.27.29‚ÄØPM.png" alt="Description of the image" style="max-width: 80%; height: auto; border-radius: 8px;">
  </div>  
  

        <!-- Lessons from Unmasking AI Section -->
        <div style="background-color: #f8f9fa; border: 1px solid #ddd; border-radius: 8px; padding: 20px;">
            <h3 style="color: #0056b3; text-align: center; margin-bottom: 10px;">Lessons from <em>Unmasking AI</em></h3>
<div class="chat-container">
  <!-- Wei's Chat Bubble 1 -->
  <div class="chat-message right">
    <img src="wei_zhang.jpg" alt="Wei Zhang" class="avatar">
    <div class="chat-bubble">
      <p>‚ùï<strong>Inclusive Datasets:</strong> Unmasking AI taught us that representation matters. By initially focusing on too narrow a dataset, we risked encoding biases. Understanding this, our final iteration prioritized a broader range of participants and recording conditions. We explicitly documented where the data came from and the types of variations included, making the training process more transparent and equitable.</p>
    </div>
  </div>

  <!-- Ziwei's Chat Bubble 1 -->
  <div class="chat-message left">
    <img src="ziwei.jpg" alt="Ziwei Yu" class="avatar">
    <div class="chat-bubble">
      <p>‚ùì<strong>Bias in Training and Testing:</strong> The book emphasizes that bias can emerge in subtle ways. For instance, a model trained in quiet environments struggles in noisy ones‚Äîan environmental bias. Our expanded dataset now includes a richer variety of noise conditions, reflecting the complexities of real-world scenarios. This approach helps mitigate bias and improves the model‚Äôs robustness, aligning our methods with Buolamwini‚Äôs call for AI that works well beyond controlled laboratory conditions.</p>
    </div>
  </div>

  <!-- Wei's Chat Bubble 2 -->
  <div class="chat-message right">
    <img src="wei_zhang.jpg" alt="Wei Zhang" class="avatar">
    <div class="chat-bubble">
      <p>‚ôøÔ∏è<strong>Accessibility and UX:</strong> Buolamwini‚Äôs critique reminds us that AI systems should benefit everyone, including those with disabilities. In response, we aimed to make our interface more intuitive by using visual and textual cues that are easy to understand. However, due to current model inaccuracies, multiple and sometimes confusing output symbols still appear in rapid succession after a single clapping sequence. This highlights the need for further model refinement rather than just interface adjustments. By improving the accuracy of our classifier, we can better serve users of diverse abilities, ensuring that the system remains truly inclusive and accessible.</p>
    </div>
  </div>

  <!-- Ziwei's Chat Bubble 2 -->
  <div class="chat-message left">
    <img src="ziwei.jpg" alt="Ziwei Yu" class="avatar">
    <div class="chat-bubble">
      <p>ü™õ<strong>Ethical Responsibility of Developers:</strong> Unmasking AI stresses that developers have an ethical duty to consider social impacts. In defining clapping gestures and classification thresholds, we moved away from overly rigid definitions. Instead, we embraced flexibility that accommodates cultural and individual differences. This ensures that the model does not impose a single ‚Äúcorrect‚Äù way of clapping, instead recognizing a spectrum of possibilities.</p>
    </div>
  </div>

  <!-- Wei's Chat Bubble 3 -->
  <div class="chat-message right">
    <img src="wei_zhang.jpg" alt="Wei Zhang" class="avatar">
    <div class="chat-bubble">
      <p>‚öôÔ∏è<strong>Broader Implications of AI:</strong> This project served as a microcosm of larger ethical debates. We learned that even a small sound classifier can reflect societal power dynamics and cultural values. By thinking critically and referencing texts like Unmasking AI, we ensure that our work does not exist in a vacuum, but rather contributes to ongoing discussions about AI‚Äôs role in shaping a more just and inclusive future.</p>
    </div>
  </div>
</div>


</div>

<!-- Insert the image -->
<div style="text-align: center; margin: 20px 0;">
    <img src="Screenshot 2024-12-09 at 5.28.29‚ÄØPM.png" alt="Description of the image" style="max-width: 80%; height: auto; border-radius: 8px;">
  </div>
  

        <!-- Future Improvements Section -->
        <div style="background-color: #fff9e6; border: 1px solid #ddd; border-radius: 8px; padding: 20px;">
            <h3 style="color: #0056b3; text-align: center; margin-bottom: 10px;">Future Improvements</h3>
            <ul style="margin-left: 20px;">
<div class="chat-container">
  <!-- Wei's Chat Bubble 1 -->
  <div class="chat-message right">
    <img src="wei_zhang.jpg" alt="Wei Zhang" class="avatar">
    <div class="chat-bubble">
      <p>üìä<strong>Expanding the Dataset:</strong> We will continue to broaden the dataset, incorporating more participants from varied linguistic and cultural communities. We will also add more nuanced types of claps‚Äîsoft, rhythmic, or asynchronous‚Äîand gather data in a wider range of acoustic environments. Such expansions will help the model better generalize and reduce cultural or situational biases.</p>
    </div>
  </div>

  <!-- Wei's Chat Bubble 2 -->
  <div class="chat-message right">
    <img src="wei_zhang.jpg" alt="Wei Zhang" class="avatar">
    <div class="chat-bubble">
      <p>üîä<strong>Enhancing Noise Robustness:</strong> Future versions will integrate more deliberate noise augmentation and test the model in public spaces, classrooms, and outdoor environments. By applying advanced noise-canceling techniques or leveraging pre-processing methods, we can further isolate clapping patterns from competing sounds.</p>
    </div>
  </div>

  <!-- Ziwei's Chat Bubble -->
  <div class="chat-message left">
    <img src="ziwei.jpg" alt="Ziwei Yu" class="avatar">
    <div class="chat-bubble">
      <p>ü§î<strong>Customization Beyond Teachable Machine:</strong> To further refine our model, we may transition from Teachable Machine to more advanced frameworks like TensorFlow or PyTorch. This change would allow for deeper customization of model architectures, feature extraction, and hyperparameters, ultimately leading to more accurate and culturally sensitive classification results.</p>
    </div>
  </div>
</div>


<!-- Insert the image -->
<div style="text-align: center; margin: 20px 0;">
    <img src="./Screenshot 2024-12-02 at 2.03.53‚ÄØPM.png" alt="Description of the image" style="max-width: 0%; height: auto; border-radius: 8px;">
  </div>
  
        <!-- Conclusion Section -->
        <div style="background-color: #f9f9f9; border: 1px solid #ddd; border-radius: 8px; padding: 20px;">
            <h3 style="color: #0056b3; text-align: center; margin-bottom: 10px;">Conclusion</h3>
    </p>
    <p>
      This project‚Äôs evolution demonstrates that even a seemingly straightforward sound classifier has rich ethical and cultural dimensions. By heeding feedback, deepening our engagement with Unmasking AI, and continuously refining our data and interface, we have created a more inclusive, transparent, and contextually aware system. The Sound Classifier now better represents the variety of ways people clap, acknowledging that no single pattern is universally ‚Äúcorrect.‚Äù In doing so, it illustrates how careful, reflective design can make AI systems not just more accurate, but more just and empathetic.
    </p>
    <p>
      Looking forward, the Sound Classifier could play a role in assistive technologies, educational tools, and entertainment platforms, benefiting individuals who rely on auditory cues and gesture-based interactions. By remaining critically engaged with the ethics and cultural implications of machine learning, we can continue shaping AI in a manner that is fair, accessible, and responsibly aligned with human values.    </p>
    <p>
</div>
</section>

<!-- Change Log Section -->
<div style="background-color: #f9f9f9; border: 1px solid #ddd; border-radius: 8px; padding: 20px; margin-bottom: 20px; font-family: Arial, sans-serif;">
  <h3 style="color: #0056b3; text-align: center; margin-bottom: 20px; border-bottom: 1px solid #ccc; padding-bottom: 10px;">Change Log</h3>

  <div style="margin-bottom: 20px;">
    <h4 style="color: #333; margin-bottom: 10px;">Data Diversity & Noise Augmentation</h4>
    <ul style="line-height: 1.6; margin-left: 20px;">
      <li>Increased the number of clapping samples per gesture from <strong>20 to 50</strong>, incorporating a wider range of participants with different genders, ages, and cultural backgrounds.</li>
      <li>Expanded background noise samples from <strong>20 to 140</strong>, including recordings from both quiet and noisy environments, to better reflect real-world conditions and reduce environmental bias.</li>
    </ul>
  </div>

  <div style="margin-bottom: 20px;">
    <h4 style="color: #333; margin-bottom: 10px;">Interface Simplification & Navigation Update</h4>
    <ul style="line-height: 1.6; margin-left: 20px;">
      <li>Decreased emoji display size for a clearer, less cluttered interface.</li>
      <li>Changed the bottom navigation icon from ‚ÄúBack to Home‚Äù to ‚ÄúReturn to Top,‚Äù streamlining the user experience.</li>
      <li>Fixed webpage styling errors to create a cleaner, more visually appealing layout.</li>
    </ul>
  </div>

  <div style="margin-bottom: 20px;">
    <h4 style="color: #333; margin-bottom: 10px;">Deeper Engagement with <em>Unmasking AI</em></h4>
    <ul style="line-height: 1.6; margin-left: 20px;">
      <li>Revised the project statement to include more nuanced discussions of cultural relevance and bias in AI.</li>
      <li>Drew direct connections between data diversity, ethical responsibilities, and the themes in Joy Buolamwini‚Äôs <em>Unmasking AI</em>.</li>
      <li>Added explicit acknowledgments of how cultural differences in clapping patterns influence model design and interpretation.</li>
      <li>Added explanatory notes on how the act of ‚Äúclapping‚Äù may hold different meanings and be expressed differently across various cultures and communities.</li>
    </ul>
  </div>

  <div>
    <h4 style="color: #333; margin-bottom: 10px;">Additional Resources</h4>
    <ul style="line-height: 1.6; margin-left: 20px;">
      <li>Included a new video demonstration embedded within the project webpage, providing a clear visual explanation of the model‚Äôs functionality and improvements.</li>
      <li>Updated the GitHub repository with the latest code.</li>
    </ul>
  </div>
</div>

<!-- Video Embedding Section -->
<div style="background-color: #fff; border: 1px solid #ddd; border-radius: 8px; padding: 20px; margin-bottom: 20px;">
  <h3 style="color: #0056b3; text-align: center; margin-bottom: 10px;">Video Demonstration</h3>
  <p style="text-align: center;">üëÄWatch the algorithm in action below‚¨áÔ∏è:</p>
  <div style="text-align: center;">
      <!-- ÂéüÊúâÁöÑ YouTube ËßÜÈ¢ë -->
      <iframe width="560" height="315" src="https://www.youtube.com/embed/TOrVsLklltM" title="Teachable Machine Sound Classifier" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      
      <!-- Êñ∞ÁöÑËßÜÈ¢ëËØ¥Êòé -->
      <p style="text-align: center; margin-top: 20px; font-size: 16px; color: #333;">
        Below is an actual demonstration of our Sound Classifier in action, recorded during our testing phase‚¨áÔ∏è:
      </p>
      
      <!-- Êñ∞Â¢ûÂä†ÁöÑÊú¨Âú∞ËßÜÈ¢ëÊñá‰ª∂ -->
      <div style="margin-top: 10px;">
        <video width="560" height="315" controls>
          <source src="video3304212740.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
  </div>
</div>
</section>


<!-- Attachments Section -->
<section id="attachments">
  <h2>Influences and References</h2>
  <div style="line-height: 1.5;">
      <p>This project drew inspiration from The Coding Train's tutorial on sound classification.</p>
      <p style="text-align: center;">You can view their tutorial ‚û°Ô∏è <a href="https://thecodingtrain.com/tracks/teachable-machine/teachable-machine/3-sound-classification" target="_blank">here</a>.</p>
  </div>

  <!-- Add GitHub Repository Links -->
  <div style="text-align: center; margin-top: 40px;">
    <h3 style="color: navy;">GitHub Repositories</h3>
    <p style="font-size: 16px;">
      Current Project Repository: 
      <a href="https://github.com/wzhang729/lis500.git" target="_blank" style="color: blue; text-decoration: underline;">https://github.com/wzhang729/lis500.git</a>
    </p>
    <p style="font-size: 16px;">
      Previous Version Repository: 
      <a href="https://github.com/sjcoombsnovii/sjcoombsnovii.github.io.git" target="_blank" style="color: blue; text-decoration: underline;">https://github.com/sjcoombsnovii/sjcoombsnovii.github.io.git</a>
    </p>
  </div>
</section>

        
    </main>


    <footer>
      <p>Created by Wei Zhang & Ziwei Yu as part of LIS500</p>
      <a href="#top">Back to Top</a>
  </footer>
  
</body>
</html>
